{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {
    "papermill": {
     "duration": 2.088705,
     "end_time": "2024-07-24T15:22:09.979427",
     "exception": false,
     "start_time": "2024-07-24T15:22:07.890722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3494b-0296-4bb8-9b07-48a55533413d",
   "metadata": {
    "papermill": {
     "duration": 0.010781,
     "end_time": "2024-07-24T15:22:10.011679",
     "exception": false,
     "start_time": "2024-07-24T15:22:10.000898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.require(\"service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e34b86-ceea-4476-88c3-a6eaaf57785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import rdBase\n",
    "blocker = rdBase.BlockLogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {
    "papermill": {
     "duration": 1.735115,
     "end_time": "2024-07-24T15:22:11.751420",
     "exception": false,
     "start_time": "2024-07-24T15:22:10.016305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m wandb login $WANDB_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c7520-0df9-4907-87f9-d7b43f5ac7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "apex_support = False\n",
    "try:\n",
    "    sys.path.append('./apex')\n",
    "    from apex import amp\n",
    "\n",
    "    apex_support = True\n",
    "except:\n",
    "    print(\"Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\")\n",
    "    apex_support = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38c4e8-7f72-42a8-a5d5-c9fa313aab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "apex_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d91b4-c4dc-4dba-a04c-e786601aab87",
   "metadata": {
    "papermill": {
     "duration": 0.004719,
     "end_time": "2024-07-24T15:22:11.761661",
     "exception": false,
     "start_time": "2024-07-24T15:22:11.756942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Upload config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4478096-04a8-4fe1-b1b9-8a8720750818",
   "metadata": {
    "papermill": {
     "duration": 0.017333,
     "end_time": "2024-07-24T15:22:11.783828",
     "exception": false,
     "start_time": "2024-07-24T15:22:11.766495",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open(\"config-gcn.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b16779-4235-42fe-9091-531e93496fbc",
   "metadata": {
    "papermill": {
     "duration": 0.020834,
     "end_time": "2024-07-24T15:22:11.809433",
     "exception": false,
     "start_time": "2024-07-24T15:22:11.788599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('batch_size =', config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa6fa0-9b19-4665-81b8-b122a0c18ceb",
   "metadata": {
    "papermill": {
     "duration": 0.342412,
     "end_time": "2024-07-24T15:22:12.157704",
     "exception": false,
     "start_time": "2024-07-24T15:22:11.815292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('running on device:', config['gpu'])\n",
    "device = torch.device(config['gpu']) if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d265d-3a4d-4ec1-b616-18805240e4e1",
   "metadata": {
    "papermill": {
     "duration": 0.011854,
     "end_time": "2024-07-24T15:22:12.175191",
     "exception": false,
     "start_time": "2024-07-24T15:22:12.163337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _save_config_file(config, log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    with open(os.path.join(log_dir, 'config.yml'), 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {
    "papermill": {
     "duration": 0.004831,
     "end_time": "2024-07-24T15:22:12.186576",
     "exception": false,
     "start_time": "2024-07-24T15:22:12.181745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {
    "papermill": {
     "duration": 44.710818,
     "end_time": "2024-07-24T15:22:56.983354",
     "exception": false,
     "start_time": "2024-07-24T15:22:12.272536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"preprocessed_10m_with_descriptors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c15fdf-f587-400b-bd31-0e55a98fbc6a",
   "metadata": {
    "papermill": {
     "duration": 3.004571,
     "end_time": "2024-07-24T15:22:59.993773",
     "exception": false,
     "start_time": "2024-07-24T15:22:56.989202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = dataframe.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {
    "papermill": {
     "duration": 0.050897,
     "end_time": "2024-07-24T15:23:00.078437",
     "exception": false,
     "start_time": "2024-07-24T15:23:00.027540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "tqdm.pandas()\n",
    "\n",
    "def string_to_array(input_string):\n",
    "    try:\n",
    "        # Use ast.literal_eval to safely evaluate the string as a Python literal\n",
    "        result = ast.literal_eval(input_string)\n",
    "        return result\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing the string: {e}\")\n",
    "        return None\n",
    "\n",
    "dataframe['descriptors'] = dataframe['descriptors'].progress_apply(lambda s: string_to_array(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d2519-d134-41b8-8250-15cd92610b8e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = dataframe.rename(columns={'smiles': 'Smiles'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377d174-01ad-49a2-a635-78b37741b469",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "# drop bad molecules\n",
    "dropping = []\n",
    "for i in tqdm(range(len(dataframe['Smiles']))):\n",
    "    mol = Chem.MolFromSmiles(dataframe['Smiles'].iloc[i])\n",
    "    if mol is None:\n",
    "        dropping.append(i)\n",
    "        continue\n",
    "    \n",
    "    if mol.GetNumAtoms() < 2:\n",
    "        dropping.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce4a4d-2911-4103-aca3-40843f0745e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(dropping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61767c2-7328-43b8-886c-0a55e4673f43",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287894eb-5d8b-4d23-ab1c-9d3a63a35b8c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Create Molecule Dataset\n",
    "##### It will generate torch_geometric.data.Data objects for both bert and GIN/GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61aaf7e-b46e-42f0-8981-67d2006f7f0d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ATOM_LIST = list(range(1,119))\n",
    "CHIRALITY_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER\n",
    "]\n",
    "BOND_LIST = [\n",
    "    Chem.rdchem.BondType.SINGLE, \n",
    "    Chem.rdchem.BondType.DOUBLE, \n",
    "    Chem.rdchem.BondType.TRIPLE, \n",
    "    Chem.rdchem.BondType.AROMATIC\n",
    "]\n",
    "BONDDIR_LIST = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d42c6-d0d0-47ef-be7a-767648a2cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b352f-4019-4358-9c6a-ad267588ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shifter as sh\n",
    "\n",
    "shifter = sh.Shifter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2fc4c-e498-4183-814e-2d33638d021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer as tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0bdc0-89c4-494d-a76a-5cc7071cca22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, node_mask_percent=0.15, edge_mask_percent=0.25):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.node_mask_percent = node_mask_percent\n",
    "        self.edge_mask_percent = edge_mask_percent\n",
    "        self.input_ids = []\n",
    "        self.mask = []\n",
    "        self.labels = []\n",
    "        \n",
    "        #self.tokenizer = tokenizer\n",
    "        #self.tokenizer.model_max_len = 512\n",
    "\n",
    "        self.dataset['graph'] = self.dataset['Smiles'].progress_apply(self.get_graph_from_smiles)\n",
    "        self.dataset['graph_copy1'] = self.dataset['graph'].progress_apply(lambda x: self.get_augmented_graph_copy(x[0], x[1], x[2], x[3], x[4]))\n",
    "        self.dataset['graph_copy2'] = self.dataset['graph'].progress_apply(lambda x: self.get_augmented_graph_copy(x[0], x[1], x[2], x[3], x[4]))\n",
    "\n",
    "        for descriptors_of_substructures in self.dataset['descriptors']:\n",
    "            shifter.shift(descriptors_of_substructures)\n",
    "\n",
    "        self.maximum = 0\n",
    "\n",
    "        for mol in self.dataset['descriptors']:\n",
    "            for substr in mol:\n",
    "                if substr == '$':\n",
    "                    continue\n",
    "                for descriptor in substr:\n",
    "                    for i in descriptor:\n",
    "                        self.maximum = max(self.maximum, i)\n",
    "        \n",
    "        self.tokenize_descriptors(self.dataset)\n",
    "        l =[]\n",
    "        inp = []\n",
    "        msk = []\n",
    "        for i in range(len(self.labels)):\n",
    "            l.append(self.labels[i])\n",
    "            inp.append(self.input_ids[i])\n",
    "            msk.append(self.mask[i])\n",
    "\n",
    "        self.dataset['labels'] = l\n",
    "        self.dataset['input_ids'] = inp\n",
    "        self.dataset['attention_mask'] = msk\n",
    "        \n",
    "        self.dataset['mlm'] = self.dataset.progress_apply(self.apply_mlm, axis=1)\n",
    "\n",
    "    def get_graph_from_smiles(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return torch.tensor([[], []], dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    0\n",
    "    \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "    \n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "        \n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        node_feat = torch.cat([x1, x2], dim=-1)\n",
    "    \n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            \n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "    \n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.long)\n",
    "        num_nodes = N\n",
    "        num_edges = M\n",
    "        return node_feat, edge_index, edge_attr, num_nodes, num_edges\n",
    "\n",
    "    def get_augmented_graph_copy(self, node_feat, edge_index, edge_attr, N, M):\n",
    "        num_mask_nodes = max([1, math.floor(self.node_mask_percent * N)])\n",
    "        \n",
    "        mask_nodes = random.sample(list(range(N)), num_mask_nodes)\n",
    "\n",
    "        node_feat_new = deepcopy(node_feat)\n",
    "        for atom_idx in mask_nodes:\n",
    "            node_feat_new[atom_idx, :] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        return Data(x=node_feat_new, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    def tokenize_descriptors(self, data):\n",
    "        sample = tokenizer.tokenize(data['descriptors'], max_length=512)\n",
    "        \n",
    "        self.labels.append(torch.tensor(sample['input_ids']))\n",
    "        self.mask.append(torch.tensor(sample['attention_mask']))\n",
    "        self.input_ids.append(self.mlm(self.labels[-1].detach().clone()))\n",
    "        \n",
    "        self.input_ids = torch.cat(self.input_ids)\n",
    "        self.mask = torch.cat(self.mask)\n",
    "        \n",
    "        self.labels = torch.cat(self.labels)\n",
    "        # return input_ids, mask, labels\n",
    "\n",
    "    def mlm(self, tensor):\n",
    "        rand = torch.rand(tensor.shape)\n",
    "        mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "        for i in range(tensor.shape[0]):\n",
    "            selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            tensor[i, selection] = 4\n",
    "        return tensor\n",
    "\n",
    "    def apply_mlm(self, sample):\n",
    "        labels = torch.tensor(sample.input_ids)\n",
    "        attention_mask = torch.tensor(sample.attention_mask)\n",
    "        input_ids = torch.tensor(sample.input_ids)\n",
    "        return Data(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset['mlm'].iloc[index], self.dataset['graph_copy1'].iloc[index], self.dataset['graph_copy2'].iloc[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self):\n",
    "        pass\n",
    "    def len(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d0346-4204-4edd-a947-de80e85a685d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MoleculeDataset(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef882144-9701-4c8e-b169-0e11113172b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(config['dataset']['valid_size'] * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=train_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=valid_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc2bf1-7318-4f29-afdb-8d3b70794bbc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, device, batch_size, temperature, use_cosine_similarity):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
    "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_similarity_function(self, use_cosine_similarity):\n",
    "        if use_cosine_similarity:\n",
    "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "            return self._cosine_simililarity\n",
    "        else:\n",
    "            return self._dot_simililarity\n",
    "\n",
    "    def _get_correlated_mask(self):\n",
    "        diag = np.eye(2 * self.batch_size)\n",
    "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
    "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
    "        mask = torch.from_numpy((diag + l1 + l2))\n",
    "        mask = (1 - mask).type(torch.bool)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot_simililarity(x, y):\n",
    "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, C, 2N)\n",
    "        # v shape: (N, 2N)\n",
    "        return v\n",
    "\n",
    "    def _cosine_simililarity(self, x, y):\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, 2N, C)\n",
    "        # v shape: (N, 2N)\n",
    "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "        return v\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        representations = torch.cat([zjs, zis], dim=0)\n",
    "\n",
    "        similarity_matrix = self.similarity_function(representations, representations)\n",
    "\n",
    "        # filter out the scores from the positive samples\n",
    "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
    "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
    "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
    "\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        logits = logits.abs() + 0.0001\n",
    "        logits = torch.log(logits)\n",
    "        logits /= self.temperature\n",
    "        \n",
    "        labels = torch.zeros(2 * self.batch_size).to(self.device).long()\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        return loss / (2 * self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "from torch import nn\n",
    "\n",
    "if config['graph_model_type'] == 'gin':\n",
    "    from graph_models.models.ginet_old import GINet as GraphModel\n",
    "elif config['graph_model_type'] == 'gcn':\n",
    "    from graph_models.models.gcn import GCN as GraphModel\n",
    "else:\n",
    "    raise ValueError('GNN model is not defined in config.')\n",
    "\n",
    "class MolecularBertGraph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularBertGraph, self).__init__()\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        roberta_config = roberta_config = RobertaConfig(\n",
    "            vocab_size=30_522,\n",
    "            max_position_embeddings=514,\n",
    "            hidden_size=768,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1\n",
    "        )\n",
    "        \n",
    "        self.bert = RobertaForMaskedLM(roberta_config).to(device)\n",
    "\n",
    "        self.graph_model = GraphModel(**config['graph_model']).to(device)\n",
    "        # self.graph_model = self._load_graph_pretrained_weights(self.graph_model)\n",
    "\n",
    "        self.out_graph_linear = torch.nn.Linear(2 * config['graph_model']['feat_dim'], \n",
    "                                                768, bias=True)\n",
    "\n",
    "        self.out_graph_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_graph_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn2_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_bert = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "        \n",
    "        self.bn2_bert = nn.BatchNorm1d(768)\n",
    "        \n",
    "        self.nt_xent_criterion = NTXentLoss(device, self.batch_size, **config['ntxent_loss'])\n",
    "\n",
    "    def forward(self, bert_batch, graph_batch1, graph_batch2):\n",
    "        bert_output = self.bert(input_ids=bert_batch['input_ids'].view(self.batch_size, -1), \n",
    "                                 attention_mask=bert_batch['attention_mask'].view(self.batch_size, -1),\n",
    "                                 labels=bert_batch['labels'].view(self.batch_size, -1), output_hidden_states=True)\n",
    "        bert_loss = bert_output.loss\n",
    "        bert_emb = bert_output.hidden_states[0][:, 0, :] # take emb for CLS token\n",
    "\n",
    "        graph_loss, hidden_states_1, hidden_states_2 = self.graph_step(graph_batch1, graph_batch2)\n",
    "        #graph_loss.backward()\n",
    "        \n",
    "        graph_emb = self.out_graph_linear(torch.cat((hidden_states_1, hidden_states_2), dim=-1))\n",
    "\n",
    "        graph_emb_projected1 = self.out_graph_projection1(graph_emb)\n",
    "        \n",
    "        graph_emb_projected_bn1 = self.bn1_graph(graph_emb_projected1)\n",
    "\n",
    "        graph_emb_projected2 = self.out_graph_projection2(torch.nn.functional.relu(graph_emb_projected_bn1))\n",
    "        \n",
    "        graph_emb_projected_bn2 = self.bn2_graph(graph_emb_projected2)\n",
    "        \n",
    "        #bert projections:\n",
    "        bert_emb_projected1 = self.out_bert_projection1(bert_emb)\n",
    "        \n",
    "        bert_emb_projected_bn1 = self.bn1_bert(bert_emb_projected1)\n",
    "\n",
    "        bert_emb_projected2 = self.out_bert_projection2(torch.nn.functional.relu(bert_emb_projected_bn1))\n",
    "        \n",
    "        bert_emb_projected_bn2 = self.bn2_bert(bert_emb_projected2)\n",
    "\n",
    "        # bimodal_loss = ((1 - self.cosine_sim(bert_emb, graph_emb))**2).mean()\n",
    "        bimodal_loss = self.nt_xent_criterion(bert_emb_projected_bn2, graph_emb_projected_bn2)\n",
    "        return bert_loss, graph_loss, bimodal_loss, graph_emb_projected_bn2, bert_emb_projected_bn2\n",
    "\n",
    "    def graph_step(self, xis, xjs):\n",
    "        # get the representations and the projections\n",
    "        ris, zis = self.graph_model(xis)  # [N,C]\n",
    "    \n",
    "        # get the representations and the projections\n",
    "        rjs, zjs = self.graph_model(xjs)  # [N,C]\n",
    "    \n",
    "        # normalize projection feature vectors\n",
    "        zis = torch.nn.functional.normalize(zis, dim=1)\n",
    "        zjs = torch.nn.functional.normalize(zjs, dim=1)\n",
    "\n",
    "        loss = self.nt_xent_criterion(zis, zjs)\n",
    "        return loss, ris, rjs\n",
    "        \n",
    "    def _load_graph_pretrained_weights(self, model):\n",
    "        try:\n",
    "            checkpoints_folder = os.path.join('MolCLR', 'ckpt', config['load_graph_model'], 'checkpoints')\n",
    "            print(os.path.join(checkpoints_folder, 'model.pth'))\n",
    "            state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'))\n",
    "            \n",
    "            model.load_state_dict(state_dict)\n",
    "            print(\"Loaded pre-trained model with success.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Pre-trained weights not found. Training from scratch.\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08055762-1a53-49e6-8cd7-71161852abe8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MolecularBertGraph().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04001413-80f4-4663-ab5d-a107e839c5c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epoch = config['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), float(config['init_lr']), \n",
    "    weight_decay=eval(config['weight_decay'])\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['epochs']-config['warm_up'], \n",
    "    eta_min=0, last_epoch=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"BERT+GCN-training-1m-32-fixed-aug-mean-init_lr:1e-6-wd:1e-7.NEW_LM\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Training (with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea2e1e-8d54-4f03-914f-6b6328b5da4f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f660cb2-97a3-4102-b85e-c6e23c84429e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = config['loss_params']['alpha']\n",
    "beta = config['loss_params']['beta']\n",
    "gamma = config['loss_params']['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96388fce-0dc6-43a3-b800-5133ea476cf2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    train_tqdm = tqdm(train_dataloader, unit=\"batch\")\n",
    "    train_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for (bert_batch, graph_batch1, graph_batch2) in train_tqdm:\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            bert_batch = bert_batch.to(device)\n",
    "            graph_batch1 = graph_batch1.to(device)\n",
    "            graph_batch2 = graph_batch2.to(device)\n",
    "    \n",
    "            bert_loss, graph_loss, bimodal_loss, ge, be = model(bert_batch, graph_batch1, graph_batch2)\n",
    "    \n",
    "            loss = alpha * bert_loss + beta * graph_loss + gamma * bimodal_loss\n",
    "            loss.backward()\n",
    "            \n",
    "\n",
    "            wandb.log({\"bert_loss/train\": bert_loss})\n",
    "            wandb.log({\"graph_loss/train\": graph_loss})\n",
    "            wandb.log({\"bimodal_loss/train\": bimodal_loss})\n",
    "            wandb.log({\"loss/train\": loss})\n",
    "    \n",
    "            bert_loss_sum += bert_loss.item()\n",
    "            graph_model_loss_sum += graph_loss.item()\n",
    "            bimodal_loss_sum += bimodal_loss.item()\n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "            optimizer.step()\n",
    "            train_tqdm.set_postfix(loss=loss.item(), bert_loss=bert_loss.item(), graph_loss=graph_loss.item(), bimodal_loss=bimodal_loss.item())\n",
    "        except:\n",
    "            continue\n",
    "    return bert_loss_sum / len(train_dataloader), graph_model_loss_sum / len(train_dataloader), bimodal_loss_sum / len(train_dataloader), loss_sum / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af92a73-77ad-4bd7-9727-7bb5337fd001",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_loop():\n",
    "    eval_tqdm = tqdm(eval_dataloader, unit=\"batch\")\n",
    "    eval_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    for (bert_batch, graph_batch1, graph_batch2) in eval_tqdm:\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            bert_batch = bert_batch.to(device)\n",
    "            graph_batch1 = graph_batch1.to(device)\n",
    "            graph_batch2 = graph_batch2.to(device)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                bert_loss, graph_loss, bimodal_loss, ge, be = model(bert_batch, graph_batch1, graph_batch2)\n",
    "    \n",
    "            loss = alpha * bert_loss + beta * graph_loss + gamma * bimodal_loss\n",
    "    \n",
    "            bert_loss_sum += bert_loss.item()\n",
    "            graph_model_loss_sum += graph_loss.item()\n",
    "            bimodal_loss_sum += bimodal_loss.item()\n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "            eval_tqdm.set_postfix(loss=loss.item(), bert_loss=bert_loss.item(), graph_loss=graph_loss.item(), bimodal_loss=bimodal_loss.item())\n",
    "        except:\n",
    "            continue\n",
    "    return bert_loss_sum / len(eval_dataloader), graph_model_loss_sum / len(eval_dataloader), bimodal_loss_sum / len(eval_dataloader), loss_sum / len(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd36a8-610d-4760-a984-79bf17e5b2e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_checkpoints_folder = os.path.join('ckpts')\n",
    "dir_name = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = os.path.join(model_checkpoints_folder, dir_name)\n",
    "_save_config_file(config, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a426df-461e-4180-9186-c5471b4df374",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_iter = 0\n",
    "valid_n_iter = 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch_counter in range(num_epoch):\n",
    "    bert_loss, graph_loss, bimodal_loss, loss = train_loop()\n",
    "    print('train', bert_loss, graph_loss, bimodal_loss, loss)\n",
    "    \n",
    "    wandb.log({\"bert_loss/train\": bert_loss}, step=epoch_counter)\n",
    "    wandb.log({\"graph_loss/train\": graph_loss}, step=epoch_counter)\n",
    "    wandb.log({\"bimodal_loss/train\": bimodal_loss}, step=epoch_counter)\n",
    "    wandb.log({\"loss/train\": loss}, step=epoch_counter)\n",
    "    \n",
    "    if (epoch_counter + 1) % config['save_every_n_epochs'] == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_{}.pth'.format(str(epoch_counter))))\n",
    "\n",
    "    #warmup for the first few epochs\n",
    "    if epoch_counter >= config['warm_up']:\n",
    "        wandb.log({\"cosine_lr_decay\": scheduler.get_last_lr()[0]}, step=epoch_counter)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "molclr1m.ipynb",
   "output_path": "output_molclr1m.ipynb",
   "parameters": {},
   "start_time": "2024-07-24T15:22:06.822290",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
