batch_size: 128                          # batch size
warm_up: 2                             # warm-up epochs
epochs: 40                             # total number of epochs

load_graph_model: False        # resume training
save_every_n_epochs: 1                  # automatic model saving frequecy

fp16_precision: False                   # float precision 16 (i.e. True/False)
init_lr: 5e-06                         # initial learning rate for Adam
weight_decay: 5e-6                       # weight decay for Adam
gpu: cuda:2                             # training GPU 


pretrained_roberta_name: subd-bert_10M
roberta_model:
  vocab_size: 30_522
  max_position_embeddings: 514
  hidden_size: 768
  num_attention_heads: 12
  num_hidden_layers: 6
  type_vocab_size: 1


graph_model_type: gcn                   # GNN backbone (i.e., gin/gcn)
graph_model: 
  num_layer: 5                          # number of graph conv layers
  emb_dim: 500                          # embedding dimension in graph conv layers
  feat_dim: 768                         # output feature dimention
  drop_ratio: 0                         # dropout ratio
  pool: mean                            # readout pooling (i.e., mean/max/add)

graph_aug: node                         # molecule graph augmentation strategy (i.e., node/subgraph/mix)
dataset:
  num_workers: 12                       # dataloader number of workers
  valid_size: 0.1                       # ratio of validation data
  test_size: 0.1

ntxent_loss:
  temperature: 0.1                      # temperature of NT-Xent loss
  use_cosine_similarity: True           # whether to use cosine similarity in NT-Xent loss (i.e. True/False)


loss_params:
  alpha: 1.0                            # weight for bert loss
  beta: 1.0                             # weight for graph_model loss
  gamma: 1.0                            # weight for bimodal loss
