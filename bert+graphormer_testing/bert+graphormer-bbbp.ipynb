{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927e74e-d4d2-4a87-962b-6560442669b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import rdBase\n",
    "blocker = rdBase.BlockLogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5ffd8-c435-43a0-9818-6c2ba42ff98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m wandb login $WANDB_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d91b4-c4dc-4dba-a04c-e786601aab87",
   "metadata": {},
   "source": [
    "### Upload config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4478096-04a8-4fe1-b1b9-8a8720750818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open(\"config-graphormer-qsar-classification.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b16779-4235-42fe-9091-531e93496fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('batch_size =', config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa6fa0-9b19-4665-81b8-b122a0c18ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('running on device:', config['gpu'])\n",
    "device = torch.device(config['gpu']) if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d265d-3a4d-4ec1-b616-18805240e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_config_file(config, log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    with open(os.path.join(log_dir, 'config.yml'), 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.concat([pd.read_csv(\"BBBP_train.csv\"), pd.read_csv(\"BBBP_test.csv\"),  pd.read_csv(\"BBBP_valid.csv\")])\n",
    "df1, df2, df3 = pd.read_csv(\"BBBP_train.csv\"), pd.read_csv(\"BBBP_test.csv\"),  pd.read_csv(\"BBBP_valid.csv\")\n",
    "train_idx = df1['num']\n",
    "test_idx = df2['num']\n",
    "val_idx = df3['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce4a4d-2911-4103-aca3-40843f0745e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "tqdm.pandas()\n",
    "\n",
    "def string_to_array(input_string):\n",
    "    try:\n",
    "        # Use ast.literal_eval to safely evaluate the string as a Python literal\n",
    "        result = ast.literal_eval(input_string)\n",
    "        return result\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing the string: {e}\")\n",
    "        return None\n",
    "\n",
    "dataframe['descriptors'] = dataframe['descriptors'].progress_apply(lambda s: string_to_array(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56a32d-ca35-4776-bafc-847837011bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "dropping = []\n",
    "for i in tqdm(range(len(dataframe['smiles']))):\n",
    "    mol = Chem.MolFromSmiles(dataframe['smiles'].iloc[i])\n",
    "    if mol is None:\n",
    "        dropping.append(i)\n",
    "        continue\n",
    "    \n",
    "    if mol.GetNumAtoms() < 2:\n",
    "        dropping.append(i)\n",
    "\n",
    "dataframe = dataframe.drop(dropping)\n",
    "dataframe.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = [x for x in train_idx if x in dataframe['num']]\n",
    "test_idx = [x for x in test_idx if x in dataframe['num']]\n",
    "val_idx = [x for x in val_idx if x in dataframe['num']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287894eb-5d8b-4d23-ab1c-9d3a63a35b8c",
   "metadata": {},
   "source": [
    "### Create Molecule Dataset\n",
    "##### It will generate torch_geometric.data.Data objects for both bert and GIN/GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61aaf7e-b46e-42f0-8981-67d2006f7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "ATOM_LIST = list(range(1,119))\n",
    "CHIRALITY_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER\n",
    "]\n",
    "BOND_LIST = [\n",
    "    Chem.rdchem.BondType.SINGLE, \n",
    "    Chem.rdchem.BondType.DOUBLE, \n",
    "    Chem.rdchem.BondType.TRIPLE, \n",
    "    Chem.rdchem.BondType.AROMATIC\n",
    "]\n",
    "BONDDIR_LIST = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d211166-ccbe-41cd-bf44-e14ab85f2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_cython_available\n",
    "if is_cython_available():\n",
    "\n",
    "    import pyximport\n",
    "\n",
    "    pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n",
    "    \n",
    "    from transformers.models.graphormer import algos_graphormer as algos_graphormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff523de8-5746-41f3-9fc0-43aa143c7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Mapping\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers.utils import is_cython_available, requires_backends\n",
    "\n",
    "\n",
    "if is_cython_available():\n",
    "    import pyximport\n",
    "\n",
    "    pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n",
    "    import sys\n",
    "    sys.path.append('algos_graphormer.so')\n",
    "    import algos_graphormer\n",
    "\n",
    "\n",
    "def convert_to_single_emb(x, offset: int = 512):\n",
    "    feature_num = x.shape[1] if len(x.shape) > 1 else 1\n",
    "    feature_offset = 1 + np.arange(0, feature_num * offset, offset, dtype=np.int64)\n",
    "    x = x + feature_offset\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess_item(item, keep_features=True):\n",
    "    requires_backends(preprocess_item, [\"cython\"])\n",
    "\n",
    "    if keep_features and \"edge_attr\" in item.keys():  # edge_attr\n",
    "        edge_attr = np.asarray(item[\"edge_attr\"], dtype=np.int64)\n",
    "    else:\n",
    "        edge_attr = np.ones((len(item[\"edge_index\"][0]), 1), dtype=np.int64)  # same embedding for all\n",
    "\n",
    "    if keep_features and \"node_feat\" in item.keys():  # input_nodes\n",
    "        node_feature = np.asarray(item[\"node_feat\"], dtype=np.int64)\n",
    "    else:\n",
    "        node_feature = np.ones((item[\"num_nodes\"], 1), dtype=np.int64)  # same embedding for all\n",
    "\n",
    "    edge_index = np.asarray(item[\"edge_index\"], dtype=np.int64)\n",
    "\n",
    "    input_nodes = convert_to_single_emb(node_feature) + 1\n",
    "    num_nodes = item[\"num_nodes\"]\n",
    "\n",
    "    if len(edge_attr.shape) == 1:\n",
    "        edge_attr = edge_attr[:, None]\n",
    "    attn_edge_type = np.zeros([num_nodes, num_nodes, edge_attr.shape[-1]], dtype=np.int64)\n",
    "    attn_edge_type[edge_index[0], edge_index[1]] = convert_to_single_emb(edge_attr) + 1\n",
    "\n",
    "    # node adj matrix [num_nodes, num_nodes] bool\n",
    "    adj = np.zeros([num_nodes, num_nodes], dtype=bool)\n",
    "    adj[edge_index[0], edge_index[1]] = True\n",
    "\n",
    "    shortest_path_result, path = algos_graphormer.floyd_warshall(adj)\n",
    "    max_dist = np.amax(shortest_path_result)\n",
    "\n",
    "    input_edges = algos_graphormer.gen_edge_input(max_dist, path, attn_edge_type)\n",
    "    attn_bias = np.zeros([num_nodes + 1, num_nodes + 1], dtype=np.single)  # with graph token\n",
    "\n",
    "    # combine\n",
    "    item[\"input_nodes\"] = input_nodes + 1  # we shift all indices by one for padding\n",
    "    item[\"attn_bias\"] = attn_bias\n",
    "    item[\"attn_edge_type\"] = attn_edge_type\n",
    "    item[\"spatial_pos\"] = shortest_path_result.astype(np.int64) + 1  # we shift all indices by one for padding\n",
    "    item[\"in_degree\"] = np.sum(adj, axis=1).reshape(-1) + 1  # we shift all indices by one for padding\n",
    "    item[\"out_degree\"] = item[\"in_degree\"]  # for undirected graph\n",
    "    item[\"input_edges\"] = input_edges + 1  # we shift all indices by one for padding\n",
    "    if \"labels\" not in item:\n",
    "        item[\"labels\"] = item[\"y\"]\n",
    "\n",
    "    return item\n",
    "\n",
    "\n",
    "class GraphormerDataCollator:\n",
    "    def __init__(self, spatial_pos_max=20, on_the_fly_processing=False):\n",
    "        if not is_cython_available():\n",
    "            raise ImportError(\"Graphormer preprocessing needs Cython (pyximport)\")\n",
    "\n",
    "        self.spatial_pos_max = spatial_pos_max\n",
    "        self.on_the_fly_processing = on_the_fly_processing\n",
    "\n",
    "    def __call__(self, features: List[dict]) -> Dict[str, Any]:\n",
    "        if self.on_the_fly_processing:\n",
    "            features = [preprocess_item(i) for i in features]\n",
    "\n",
    "        if not isinstance(features[0], Mapping):\n",
    "            features = [vars(f) for f in features]\n",
    "        batch = {}\n",
    "\n",
    "        max_node_num = max(len(i[\"input_nodes\"]) for i in features)\n",
    "        node_feat_size = len(features[0][\"input_nodes\"][0])\n",
    "        edge_feat_size = len(features[0][\"attn_edge_type\"][0][0])\n",
    "        max_dist = max(len(i[\"input_edges\"][0][0]) for i in features)\n",
    "        edge_input_size = len(features[0][\"input_edges\"][0][0][0])\n",
    "        batch_size = len(features)\n",
    "\n",
    "        batch[\"attn_bias\"] = torch.zeros(batch_size, max_node_num + 1, max_node_num + 1, dtype=torch.float)\n",
    "        batch[\"attn_edge_type\"] = torch.zeros(batch_size, max_node_num, max_node_num, edge_feat_size, dtype=torch.long)\n",
    "        batch[\"spatial_pos\"] = torch.zeros(batch_size, max_node_num, max_node_num, dtype=torch.long)\n",
    "        batch[\"in_degree\"] = torch.zeros(batch_size, max_node_num, dtype=torch.long)\n",
    "        batch[\"input_nodes\"] = torch.zeros(batch_size, max_node_num, node_feat_size, dtype=torch.long)\n",
    "        batch[\"input_edges\"] = torch.zeros(\n",
    "            batch_size, max_node_num, max_node_num, max_dist, edge_input_size, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        for ix, f in enumerate(features):\n",
    "            for k in [\"attn_bias\", \"attn_edge_type\", \"spatial_pos\", \"in_degree\", \"input_nodes\", \"input_edges\"]:\n",
    "                f[k] = torch.tensor(f[k])\n",
    "\n",
    "            if len(f[\"attn_bias\"][1:, 1:][f[\"spatial_pos\"] >= self.spatial_pos_max]) > 0:\n",
    "                f[\"attn_bias\"][1:, 1:][f[\"spatial_pos\"] >= self.spatial_pos_max] = float(\"-inf\")\n",
    "\n",
    "            batch[\"attn_bias\"][ix, : f[\"attn_bias\"].shape[0], : f[\"attn_bias\"].shape[1]] = f[\"attn_bias\"]\n",
    "            batch[\"attn_edge_type\"][ix, : f[\"attn_edge_type\"].shape[0], : f[\"attn_edge_type\"].shape[1], :] = f[\n",
    "                \"attn_edge_type\"\n",
    "            ]\n",
    "            batch[\"spatial_pos\"][ix, : f[\"spatial_pos\"].shape[0], : f[\"spatial_pos\"].shape[1]] = f[\"spatial_pos\"]\n",
    "            batch[\"in_degree\"][ix, : f[\"in_degree\"].shape[0]] = f[\"in_degree\"]\n",
    "            batch[\"input_nodes\"][ix, : f[\"input_nodes\"].shape[0], :] = f[\"input_nodes\"]\n",
    "            batch[\"input_edges\"][\n",
    "                ix, : f[\"input_edges\"].shape[0], : f[\"input_edges\"].shape[1], : f[\"input_edges\"].shape[2], :\n",
    "            ] = f[\"input_edges\"]\n",
    "\n",
    "        batch[\"out_degree\"] = batch[\"in_degree\"]\n",
    "\n",
    "        sample = features[0][\"labels\"]\n",
    "        if len(sample) == 1:  # one task\n",
    "            if isinstance(sample[0], float):  # regression\n",
    "                batch[\"labels\"] = torch.from_numpy(np.concatenate([i[\"labels\"] for i in features]))\n",
    "            else:  # binary classification\n",
    "                batch[\"labels\"] = torch.from_numpy(np.concatenate([i[\"labels\"] for i in features]))\n",
    "        else:  # multi task classification, left to float to keep the NaNs\n",
    "            batch[\"labels\"] = torch.from_numpy(np.stack([i[\"labels\"] for i in features], axis=0))\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef8fbe-29f0-41ee-9a4f-572f59561e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getshortest_path(datapoint):\n",
    "    num_nodes=len(datapoint['node_feat'])\n",
    "    edge_index = datapoint['edge_index']\n",
    "    adj = np.zeros([num_nodes, num_nodes], dtype=bool)\n",
    "    adj[edge_index[0], edge_index[1]] = True\n",
    "    shortest_path_result, path = algos_graphormer.floyd_warshall(adj)\n",
    "    max_dist = np.amax(shortest_path_result)\n",
    "    return {\"max_dist\":max_dist, \"path\": path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d31bd-289e-48cd-b09d-1398bf74b283",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def processItemForGraphormer(graph, yi):\n",
    "    processed = preprocess_item(\n",
    "                 {\"node_feat\":graph.x.tolist(),\n",
    "                 \"edge_index\":graph.edge_index.tolist(),\n",
    "                 \"edge_attr\":graph.edge_attr.tolist(),\n",
    "                 \"num_nodes\":len(graph.x),\n",
    "                 'y': yi\n",
    "                })\n",
    "    processed['attn_edge_type_ORIG'] = np.array(processed['attn_edge_type'])+0\n",
    "    processed['input_nodes_ORIG'] = np.array(processed['input_nodes'])+0\n",
    "    return processed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9cf49a-ef3f-48d6-8829-f8ff5a950673",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'p_np'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977ab0a-d0ab-44d6-8106-6c3722876f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, node_mask_percent=0.15, edge_mask_percent=0.2):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.node_mask_percent = node_mask_percent\n",
    "        self.edge_mask_percent = edge_mask_percent\n",
    "        self.input_ids = []\n",
    "        self.mask = []\n",
    "        self.labels = []\n",
    "        \n",
    "        \n",
    "        self.yi = torch.tensor(np.full(( 768), 1).tolist()) \n",
    "        \n",
    "        self.dataset['graph'] = self.dataset['smiles'].progress_apply(self.get_graph_from_smiles)\n",
    "        \n",
    "    \n",
    "        self.dataset['graphormerdata'] = self.dataset['graph'].progress_apply(\n",
    "              lambda graph:   processItemForGraphormer(graph, self.yi)                    \n",
    "        )                                                     \n",
    "                                                                     \n",
    "        self.dataset['graphormerdataRAW'] = self.dataset['graph'].progress_apply(\n",
    "              lambda graph:\n",
    "#       preprocess_item(\n",
    "                 {\"node_feat\":graph.x.tolist(),\n",
    "                 \"edge_index\":graph.edge_index.tolist(),\n",
    "                 \"edge_attr\":graph.edge_attr.tolist(),\n",
    "                 \"num_nodes\":len(graph.x),\n",
    "                 'y': self.yi\n",
    "                }\n",
    "#         )                      \n",
    "        ) \n",
    "        \n",
    "        self.dataset['shortest_path'] = self.dataset['graphormerdataRAW'].progress_apply(\n",
    "                   lambda datapoint:\n",
    "                        getshortest_path(datapoint)\n",
    "                    )\n",
    "        \n",
    "        self.maskedGraphAtom = torch.tensor([[len(ATOM_LIST),0]],dtype=torch.long)\n",
    "        self.edgeGraphMask = torch.tensor([len(BOND_LIST) + 1, len(BONDDIR_LIST)], dtype=torch.long)\n",
    "\n",
    "        for descriptors_of_substructures in self.dataset['descriptors']:\n",
    "            shifter.shift(descriptors_of_substructures)\n",
    "\n",
    "        self.maximum = 0\n",
    "\n",
    "        for mol in self.dataset['descriptors']:\n",
    "            for substr in mol:\n",
    "                if substr == '$':\n",
    "                    continue\n",
    "                for descriptor in substr:\n",
    "                    for i in descriptor:\n",
    "                        self.maximum = max(self.maximum, i)\n",
    "        \n",
    "        self.tokenize_descriptors(self.dataset)\n",
    "        l =[]\n",
    "        inp = []\n",
    "        msk = []\n",
    "        for i in range(len(self.labels)):\n",
    "            l.append(self.labels[i])\n",
    "            inp.append(self.input_ids[i])\n",
    "            msk.append(self.mask[i])\n",
    "\n",
    "        self.dataset['labels'] = l\n",
    "        self.dataset['input_ids'] = inp\n",
    "        self.dataset['attention_mask'] = msk\n",
    "        \n",
    "        self.dataset['mlm'] = self.dataset.progress_apply(self.apply_mlm, axis=1)\n",
    "        self.dataset['tokens'] = self.dataset['mlm']\n",
    " \n",
    "    def get_graph_from_smiles(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return torch.tensor([[], []], dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    0\n",
    "    \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "    \n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "        \n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        node_feat = torch.cat([x1, x2], dim=-1)\n",
    "    \n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            \n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "    \n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.long)\n",
    "        num_nodes = N\n",
    "        num_edges = M\n",
    "        return Data(x=node_feat, edge_index=edge_index, edge_attr=edge_attr)\n",
    "         \n",
    "\n",
    "    def get_augmented_graph_copy(self, node_feat, edge_index, edge_attr, N, M):\n",
    "        num_mask_nodes = max([1, math.floor(self.node_mask_percent * N)])\n",
    "        #num_mask_nodes = 1\n",
    "        num_mask_edges = max([0, math.floor(self.edge_mask_percent * M)])\n",
    "\n",
    "        \n",
    "        mask_nodes = random.sample(list(range(N)), num_mask_nodes)\n",
    "        mask_edges_single = random.sample(list(range(M)), num_mask_edges)\n",
    "        \n",
    "        \n",
    "        mask_edges = [2*i for i in mask_edges_single] + [2*i+1 for i in mask_edges_single]\n",
    "\n",
    "        \n",
    "        node_feat_new = deepcopy(node_feat)\n",
    "        \n",
    "        node_feat_new[mask_nodes] = (node_feat_new[mask_nodes][:,:]*0 + self.maskedGraphAtom)\n",
    "            \n",
    "        edge_attr_new = edge_attr\n",
    "        edge_attr_new[mask_edges] =  self.edgeGraphMask\n",
    "\n",
    "        return Data(x=node_feat_new, edge_index=edge_index, edge_attr=edge_attr_new)\n",
    "\n",
    "    def tokenize(self, item):\n",
    "        sample = self.tokenizer(item, truncation=True, max_length=512, padding='max_length')\n",
    "        return (torch.tensor(sample.input_ids), \n",
    "                torch.tensor(sample.attention_mask), \n",
    "                torch.tensor(sample.input_ids)\n",
    "               )\n",
    "\n",
    "    def tokenize_descriptors(self, data):\n",
    "        sample = tokenizer.tokenize(data['descriptors'], max_length=512)\n",
    "        \n",
    "        self.labels.append(torch.tensor(sample['input_ids']))\n",
    "        self.mask.append(torch.tensor(sample['attention_mask']))\n",
    "        self.input_ids.append(self.mlm(self.labels[-1].detach().clone()))\n",
    "        \n",
    "        self.input_ids = torch.cat(self.input_ids)\n",
    "        self.mask = torch.cat(self.mask)\n",
    "        \n",
    "        self.labels = torch.cat(self.labels)\n",
    "\n",
    "    def mlm(self, tensor):\n",
    "        rand = torch.rand(tensor.shape)\n",
    "        mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "        for i in range(tensor.shape[0]):\n",
    "            selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            tensor[i, selection] = 4\n",
    "        return tensor\n",
    "\n",
    "    def apply_mlm(self, sample):\n",
    "        labels = torch.tensor(sample.input_ids)\n",
    "        attention_mask = torch.tensor(sample.attention_mask)\n",
    "        input_ids = torch.tensor(sample.input_ids)\n",
    "        return Data(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bert = self.dataset['mlm'][index]\n",
    "        graph = self.dataset['graph'][index]\n",
    "        \n",
    "        return graph, bert\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self):\n",
    "        pass\n",
    "    def len(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d0346-4204-4edd-a947-de80e85a685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MoleculeDataset(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd65976-aaa9-40c2-b121-44747fde8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(val_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfa086-1a81-4a8e-99e4-d3912ff75788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_to_shape(tensor, target_shape):\n",
    "    current_shape = tensor.shape\n",
    "    num_dims = len(current_shape)\n",
    "    \n",
    "    if num_dims != len(target_shape):\n",
    "        raise ValueError(f\"Tensor has {num_dims} dimensions but target shape has {len(target_shape)} dimensions.\")\n",
    "    \n",
    "    padding = []\n",
    "    for i in range(num_dims - 1, -1, -1): \n",
    "        if target_shape[i] < current_shape[i]:\n",
    "            raise ValueError(f\"Target shape at dimension {i} is smaller than the tensor shape.\")\n",
    "        padding.append(0)\n",
    "        padding.append(target_shape[i] - current_shape[i])\n",
    "        \n",
    "    padded_tensor = F.pad(tensor, padding)\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6b542-89c9-4199-92be-1b74492549b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b50c2-aa12-447a-92c2-e96be54a9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class CustomBatchDataset(IterableDataset):\n",
    "    \n",
    "    def setTrain(self,train = True):\n",
    "        if train:\n",
    "            self.sample_id = self.train_idx\n",
    "            self.train = True\n",
    "        else:\n",
    "            self.sample_id = self.valid_idx\n",
    "            self.train = False\n",
    "    \n",
    "    def __init__(self, dataframe,dataset, train_idx,valid_idx, batch_size):\n",
    "        \n",
    "        self.y = torch.tensor(np.full((config['batch_size'] * 2, 768), 1).tolist())\n",
    "        self.yi = torch.tensor(np.full(( 768), 1).tolist())\n",
    "        self.dataset=dataset\n",
    "        self.dataframe = dataframe\n",
    "        self.targets = dataframe[target_col].to_numpy()\n",
    "        self.sample_id = train_idx\n",
    "        self.valid_idx = valid_idx\n",
    "        self.train_idx = train_idx\n",
    "        self.batch_size = batch_size\n",
    "        self.train = True\n",
    "        \n",
    "        input_ids = [e['input_ids'] for e in dataframe['tokens']]\n",
    "        self.input_ids = torch.stack(input_ids)\n",
    "\n",
    "        attention_mask = [e['attention_mask'] for e in dataframe['tokens']]\n",
    "        self.attention_mask = torch.stack(attention_mask)\n",
    "\n",
    "        labels = [e['labels'] for e in dataframe['tokens']]\n",
    "        self.labels = torch.stack(labels)\n",
    "\n",
    "        self.graphs = [e for e in dataframe['graph']]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Custom iterator that yields batches of data and labels.\n",
    "        \"\"\"\n",
    "        # Get the total number of samples\n",
    "        total_samples = (len(self.sample_id)//self.batch_size)*self.batch_size\n",
    "        if self.train:\n",
    "            np.random.shuffle(self.sample_id)\n",
    "        # Yield minibatches\n",
    "        for i in range(0, total_samples, self.batch_size):\n",
    "             \n",
    "            S = self.sample_id[ i : i + self.batch_size]\n",
    "\n",
    "            return_targets = self.targets[S]\n",
    "            \n",
    "            inp_Idx =  self.input_ids[S]\n",
    "            rand = torch.rand(inp_Idx.shape)\n",
    "            mask_arr = (rand < .15) * (inp_Idx != 0) * (inp_Idx != 1) * (inp_Idx != 2)\n",
    "            inp_Idx[mask_arr] = 4\n",
    "            atte = self.attention_mask[S]\n",
    "            labe = self.labels[S]\n",
    "            \n",
    "            SS = S+S\n",
    "            graphdataProcessed =  [ self.dataframe.graphormerdata[i] for i in SS]\n",
    "\n",
    "            shortestPath =  [ self.dataframe.shortest_path[i] for i in SS]\n",
    "            selId = [ \n",
    "                    random.sample(\n",
    "                     list(range(len(g[\"node_feat\"]))),  \n",
    "                                      max([1, math.floor(self.dataset.node_mask_percent \n",
    "                                                         * len(g[\"node_feat\"]))])\n",
    "                             )\n",
    "                                            for g  in  graphdataProcessed ]\n",
    "             \n",
    "            \n",
    "            for g, selidi in zip(graphdataProcessed,selId):\n",
    "                g[\"input_nodes\"]= g[\"input_nodes_ORIG\"] +0\n",
    "                for s in selidi:\n",
    "                    g[\"input_nodes\"][s,0]=self.dataset.maskedGraphAtom[0][0]\n",
    "            \n",
    "            eselId = [ \n",
    "                 random.sample(\n",
    "                                 list(range(len(g['edge_index'][0])//2)),  \n",
    "                                  max([0, math.floor(self.dataset.edge_mask_percent * (len(g['edge_index'][0])//2))])\n",
    "                         )\n",
    "                                        for g   in   graphdataProcessed ]\n",
    "            for g, eid  in zip(graphdataProcessed, eselId):\n",
    "                g['attn_edge_type'] = np.array(g['attn_edge_type_ORIG'])+0\n",
    "                for e in eid:\n",
    "                    fn = g['edge_index'][0][2*e]\n",
    "                    tn = g['edge_index'][1][2*e]\n",
    "                    g['attn_edge_type'][fn,tn,:] = self.dataset.edgeGraphMask\n",
    "                    fn = g['edge_index'][0][2*e+1]\n",
    "                    tn = g['edge_index'][1][2*e+1]\n",
    "                    g['attn_edge_type'][fn,tn,:] = self.dataset.edgeGraphMask\n",
    "            \n",
    "            \n",
    "            for g, sp  in zip(graphdataProcessed, shortestPath):\n",
    "                edge_attr = g['attn_edge_type']\n",
    "                \n",
    "                input_edges = algos_graphormer.gen_edge_input(sp['max_dist'], sp['path'], edge_attr)\n",
    "\n",
    "                g['input_edges'] = input_edges+1\n",
    "            \n",
    "            yield inp_Idx, atte, labe, graphdataProcessed, return_targets#, g1, g2 \n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_idx)//self.batch_size\n",
    "        else:\n",
    "            return len(self.valid_idx)//self.batch_size\n",
    "\n",
    "\n",
    "custom_batch_dataset = CustomBatchDataset(dataframe, dataset, train_idx,valid_idx, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {},
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc2bf1-7318-4f29-afdb-8d3b70794bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, device, batch_size, temperature, use_cosine_similarity):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
    "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_similarity_function(self, use_cosine_similarity):\n",
    "        if use_cosine_similarity:\n",
    "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "            return self._cosine_simililarity\n",
    "        else:\n",
    "            return self._dot_simililarity\n",
    "\n",
    "    def _get_correlated_mask(self):\n",
    "        diag = np.eye(2 * self.batch_size)\n",
    "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
    "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
    "        mask = torch.from_numpy((diag + l1 + l2))\n",
    "        mask = (1 - mask).type(torch.bool)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot_simililarity(x, y):\n",
    "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, C, 2N)\n",
    "        # v shape: (N, 2N)\n",
    "        return v\n",
    "\n",
    "    def _cosine_simililarity(self, x, y):\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, 2N, C)\n",
    "        # v shape: (N, 2N)\n",
    "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "        return v\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        representations = torch.cat([zjs, zis], dim=0)\n",
    "\n",
    "        similarity_matrix = self.similarity_function(representations, representations)\n",
    "\n",
    "        # filter out the scores from the positive samples\n",
    "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
    "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
    "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
    "\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        logits = logits.abs() + 0.0001\n",
    "        logits = torch.log(logits)\n",
    "        logits /= self.temperature\n",
    "        \n",
    "        labels = torch.zeros(2 * self.batch_size).to(self.device).long()\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        return loss / (2 * self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b978884-bcc8-4171-9c70-8d73ebfd4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.graphormer.collating_graphormer import GraphormerDataCollator\n",
    "from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator\n",
    "from transformers import GraphormerForGraphClassification\n",
    "\n",
    "class GraphormerDataCollator_():\n",
    "    def __init__(self):\n",
    "        self.data_collator = GraphormerDataCollator()\n",
    "\n",
    "    def __call__(self, features):\n",
    "        for mol in features:\n",
    "            if mol['num_nodes'] == 1:\n",
    "                features.remove(mol)\n",
    "        return self.data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "from torch import nn\n",
    "\n",
    "class MolecularBertGraph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularBertGraph, self).__init__()\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        roberta_config = RobertaConfig(\n",
    "            vocab_size=30_522,\n",
    "            max_position_embeddings=514,\n",
    "            hidden_size=768,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1\n",
    "        )\n",
    "        self.bert = RobertaForMaskedLM(roberta_config)\n",
    "        \n",
    "        self.data_collator = GraphormerDataCollator_()\n",
    "        \n",
    "        self.graph_model = GraphormerForGraphClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_classes=1,\n",
    "            ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "            ).to(device)     # GraphModel(**config['model'])\n",
    "        self.graph_model.classifier = nn.Identity()\n",
    "\n",
    "        self.out_graph_linear = torch.nn.Linear(768 * 2, 768, bias=True)\n",
    "\n",
    "        self.out_graph_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_graph_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn2_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_bert = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "        \n",
    "        self.bn2_bert = nn.BatchNorm1d(768)\n",
    "\n",
    "        # contrastive loss for MolCLR\n",
    "        self.nt_xent_criterion = NTXentLoss(device, self.batch_size, **config['loss'])\n",
    "        # cosine distance as loss between models\n",
    "        self.cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, inp_Idx, atte, labe, graphdataProcessed):\n",
    "        bert_output = self.bert(input_ids=inp_Idx, \n",
    "                                 attention_mask=atte,\n",
    "                                 labels=labe, output_hidden_states=True)\n",
    "        bert_loss = bert_output.loss\n",
    "        bert_emb = bert_output.hidden_states[0][:, 0, :] # take emb for CLS token\n",
    "\n",
    "        graph_loss, hidden_states_1, hidden_states_2 = self.graph_step(graphdataProcessed)\n",
    "    \n",
    "        graph_emb = self.out_graph_linear(torch.cat((hidden_states_1, hidden_states_2), dim=-1)).mean(axis=0)\n",
    "        graph_emb_projected1 = self.out_graph_projection1(graph_emb)\n",
    "        graph_emb_projected_bn1 = self.bn1_graph(graph_emb_projected1)\n",
    "        graph_emb_projected2 = self.out_graph_projection2(torch.nn.functional.relu(graph_emb_projected_bn1))\n",
    "        graph_emb_projected_bn2 = self.bn2_graph(graph_emb_projected2)\n",
    "        #bert projections:\n",
    "        bert_emb_projected1 = self.out_bert_projection1(bert_emb)\n",
    "        bert_emb_projected_bn1 = self.bn1_bert(bert_emb_projected1)\n",
    "        bert_emb_projected2 = self.out_bert_projection2(torch.nn.functional.relu(bert_emb_projected_bn1))\n",
    "        bert_emb_projected_bn2 = self.bn2_bert(bert_emb_projected2)\n",
    "        bimodal_loss = self.nt_xent_criterion(bert_emb_projected_bn2, graph_emb_projected_bn2)\n",
    "        return bert_loss, graph_loss, bimodal_loss, graph_emb_projected_bn2, bert_emb_projected_bn2\n",
    "\n",
    "    def graph_step(self, graphdataProcessed):\n",
    "        batch = {}\n",
    "        for k in ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']:\n",
    "            shp = np.max([  np.array(e[k]).shape for e in graphdataProcessed], 0)\n",
    "            batch[k] = torch.stack([pad_to_shape(torch.tensor(e[k]), shp) for e in graphdataProcessed])\n",
    "        \n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() }\n",
    "        outputs = self.graph_model(**input_batch)\n",
    "        # get the representations and the projections\n",
    "        zis = outputs.logits[:config['batch_size']]\n",
    "        zjs = outputs.logits[config['batch_size']:]\n",
    "\n",
    "        ris = outputs.hidden_states[0][:, 0:config['batch_size'], :].to(device)\n",
    "        rjs = outputs.hidden_states[0][:, config['batch_size']:config['batch_size']*2, :].to(device)\n",
    "        \n",
    "        zis = torch.nn.functional.normalize(zis, dim=1)\n",
    "        zjs = torch.nn.functional.normalize(zjs, dim=1)\n",
    "         \n",
    "        loss = self.nt_xent_criterion(zis, zjs)\n",
    "        return loss, ris, rjs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"bert_graphormer\",\n",
    "    name=\"graphormer BBBP QSAR-all metrics-fast-18ep\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training (with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea2e1e-8d54-4f03-914f-6b6328b5da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd36a8-610d-4760-a984-79bf17e5b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_checkpoints_folder = os.path.join('ckpts')\n",
    "dir_name = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = os.path.join(model_checkpoints_folder, dir_name)\n",
    "_save_config_file(config, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f9354-6918-4529-a168-d55643db56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_base = 'graphormer-base-pcqm4mv1'\n",
    "model_name = 'clefourrier/graphormer-base-pcqm4mv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df3a5c-965c-4a66-bd38-a8158398ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularPropertiesClassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularPropertiesClassification, self).__init__()\n",
    "\n",
    "        self.test = MolecularBertGraph().to(device)\n",
    "        self.test.load_state_dict(torch.load('weights_pretr/model_17.pth'))\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(768 * 2, 768, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(768, 2, bias=True)\n",
    "\n",
    "    def forward(self,inp_idx, atte, labe, graphdataProcessed):\n",
    "        l1, l2, l3, graph_emb, bert_emb = self.test(inp_idx, atte, labe, graphdataProcessed)\n",
    "        \n",
    "        first_linear_out = self.linear1(torch.cat((graph_emb, bert_emb), dim=-1))\n",
    "        logits = self.linear2(torch.nn.functional.sigmoid(first_linear_out))\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca196530-c8af-4581-b213-affedb0109db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MolecularPropertiesClassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3826d7c-9d81-4c05-87f3-9bbde4986f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877c631-0ca2-4527-a2d3-009122d28b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = config['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), config['init_lr'], \n",
    "    weight_decay=eval(config['weight_decay'])\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['epochs']-config['warm_up'], \n",
    "    eta_min=0, last_epoch=-1\n",
    ")\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c431e43-da30-40d3-bff0-4e06bfde9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    custom_batch_dataset.setTrain(True)\n",
    "    train_tqdm = tqdm(custom_batch_dataset, unit=\"batch\")\n",
    "    train_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    \n",
    "    model.train()\n",
    "    for inp_Idx, atte, labe, graphdataProcessed, targets in train_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inp_Idx = inp_Idx.to(device)\n",
    "        atte = atte.to(device)\n",
    "        labe = labe.to(device)\n",
    "        targets = torch.tensor(targets).to(device)\n",
    "\n",
    "        logits = model(inp_Idx, atte, labe, graphdataProcessed)\n",
    "\n",
    "        loss = loss_func(logits.view(-1, 2), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        pred_labels = torch.argmax(logits, dim=-1)\n",
    "        true_labels = targets\n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_tqdm.set_postfix(loss=loss.item())\n",
    "    return loss_sum / len(custom_batch_dataset), total_pred_labels, total_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc579384-660b-4979-80ce-199a40c953e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop():\n",
    "    custom_batch_dataset.setTrain(False)\n",
    "    eval_tqdm = tqdm(custom_batch_dataset, unit=\"batch\")\n",
    "    eval_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    for inp_Idx, atte, labe, graphdataProcessed, targets in eval_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inp_Idx = inp_Idx.to(device)\n",
    "        atte = atte.to(device)\n",
    "        labe = labe.to(device)\n",
    "        targets = torch.tensor(targets).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp_Idx, atte, labe, graphdataProcessed)\n",
    "\n",
    "        loss = loss_func(logits.view(-1, 2), targets.view(-1))\n",
    "\n",
    "        pred_labels = torch.argmax(logits, dim=-1)\n",
    "        true_labels = targets\n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        eval_tqdm.set_postfix(loss=loss.item())\n",
    "    return loss_sum / len(custom_batch_dataset), total_pred_labels, total_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1df98b-7cfb-4841-87a5-36f767b6823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423f7a4-5757-43b0-8f17-23f1fd58da2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iter = 0\n",
    "valid_n_iter = 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch_counter in range(num_epoch):\n",
    "    loss, total_pred_labels, total_true_labels = train_loop()\n",
    "    \n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "    wandb.log({\"loss/train\": loss})\n",
    "    wandb.log({\"accuracy/train\": accuracy_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"f1/train\": f1_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"precision/train\": precision_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"recall/train\": recall_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_auc_score/train\": roc_auc_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_curve/train\": roc_curve(total_true_labels, total_pred_labels)})\n",
    "\n",
    "    loss, total_pred_labels, total_true_labels = eval_loop()\n",
    "\n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "    wandb.log({\"loss/eval\": loss})\n",
    "    wandb.log({\"accuracy/validation\": accuracy_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"f1/validation\": f1_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"precision/validation\": precision_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"recall/validation\": recall_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_auc_score/validation\": roc_auc_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_curve/validation\": roc_curve(total_true_labels, total_pred_labels)})\n",
    "    \n",
    "    if loss < best_valid_loss:\n",
    "        best_valid_loss = loss\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_qsar_bbbp.pth'))\n",
    "    \n",
    "    if (epoch_counter + 1) % config['save_every_n_epochs'] == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_qsar_bbbp_{}.pth'.format(str(epoch_counter))))\n",
    "\n",
    "    if epoch_counter >= config['warm_up']:\n",
    "                scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
