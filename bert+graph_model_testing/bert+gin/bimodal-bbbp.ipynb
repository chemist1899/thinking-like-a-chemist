{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927e74e-d4d2-4a87-962b-6560442669b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import rdBase\n",
    "blocker = rdBase.BlockLogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m wandb login $WANDB_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d91b4-c4dc-4dba-a04c-e786601aab87",
   "metadata": {},
   "source": [
    "### Upload config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4478096-04a8-4fe1-b1b9-8a8720750818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open(\"config-qsar-classification-exact.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b16779-4235-42fe-9091-531e93496fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('batch_size =', config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa6fa0-9b19-4665-81b8-b122a0c18ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('running on device:', config['gpu'])\n",
    "device = torch.device(config['gpu']) if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d265d-3a4d-4ec1-b616-18805240e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_config_file(config, log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    with open(os.path.join(log_dir, 'config.yml'), 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.concat([pd.read_csv(\"BBBP_train.csv\"), pd.read_csv(\"BBBP_test.csv\"),  pd.read_csv(\"BBBP_valid.csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720d630-c5f1-4443-ad01-2e21e3c90147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, df2, df3 = pd.read_csv(\"BBBP_train.csv\"), pd.read_csv(\"BBBP_test.csv\"),  pd.read_csv(\"BBBP_valid.csv\")\n",
    "train_idx = df1['num']\n",
    "test_idx = df2['num']\n",
    "val_idx = df3['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56a32d-ca35-4776-bafc-847837011bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "tqdm.pandas()\n",
    "\n",
    "def string_to_array(input_string):\n",
    "    try:\n",
    "        # Use ast.literal_eval to safely evaluate the string as a Python literal\n",
    "        result = ast.literal_eval(input_string)\n",
    "        return result\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing the string: {e}\")\n",
    "        return None\n",
    "\n",
    "dataframe['descriptors'] = dataframe['descriptors'].progress_apply(lambda s: string_to_array(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca17b71-a4eb-4fdf-b760-0a5c815dda87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "dropping = []\n",
    "for i in tqdm(range(len(dataframe['smiles']))):\n",
    "    mol = Chem.MolFromSmiles(dataframe['smiles'].iloc[i])\n",
    "    if mol is None:\n",
    "        dropping.append(i)\n",
    "        continue\n",
    "    \n",
    "    if mol.GetNumAtoms() < 2:\n",
    "        dropping.append(i)\n",
    "\n",
    "dataframe = dataframe.drop(dropping)\n",
    "dataframe.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9010cf-8b66-4d1c-830f-a481eebeefcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = [x for x in train_idx if x in dataframe['num']]\n",
    "test_idx = [x for x in test_idx if x in dataframe['num']]\n",
    "val_idx = [x for x in val_idx if x in dataframe['num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef808f-5c97-4e67-b943-010d6e7fa6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287894eb-5d8b-4d23-ab1c-9d3a63a35b8c",
   "metadata": {},
   "source": [
    "### Create Molecule Dataset\n",
    "##### It will generate torch_geometric.data.Data objects for both bert and GIN/GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61aaf7e-b46e-42f0-8981-67d2006f7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "ATOM_LIST = list(range(1,119))\n",
    "CHIRALITY_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER\n",
    "]\n",
    "BOND_LIST = [\n",
    "    Chem.rdchem.BondType.SINGLE, \n",
    "    Chem.rdchem.BondType.DOUBLE, \n",
    "    Chem.rdchem.BondType.TRIPLE, \n",
    "    Chem.rdchem.BondType.AROMATIC\n",
    "]\n",
    "BONDDIR_LIST = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c523406-d8d6-4556-a85b-95d2291af7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0bdc0-89c4-494d-a76a-5cc7071cca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import shifter as sh\n",
    "shifter = sh.Shifter()\n",
    "import tokenizer as tokenizer\n",
    "\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, node_mask_percent=0.15, edge_mask_percent=0.25):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.node_mask_percent = node_mask_percent\n",
    "        self.edge_mask_percent = edge_mask_percent\n",
    "        self.input_ids = []\n",
    "        self.mask = []\n",
    "        self.labels = []\n",
    "        \n",
    "\n",
    "        self.dataset['graph'] = self.dataset['smiles'].progress_apply(self.get_graph_from_smiles)\n",
    "        self.dataset['graph_copy1'] = self.dataset['graph'].progress_apply(lambda x: self.get_augmented_graph_copy(x[0], x[1], x[2], x[3], x[4]))\n",
    "        self.dataset['graph_copy2'] = self.dataset['graph'].progress_apply(lambda x: self.get_augmented_graph_copy(x[0], x[1], x[2], x[3], x[4]))\n",
    "\n",
    "\n",
    "        for descriptors_of_substructures in self.dataset['descriptors']:\n",
    "            shifter.shift(descriptors_of_substructures)\n",
    "\n",
    "        self.maximum = 0\n",
    "\n",
    "        for mol in self.dataset['descriptors']:\n",
    "            for substr in mol:\n",
    "                if substr == '$':\n",
    "                    continue\n",
    "                for descriptor in substr:\n",
    "                    for i in descriptor:\n",
    "                        self.maximum = max(self.maximum, i)\n",
    "        \n",
    "        self.tokenize_descriptors(self.dataset)\n",
    "        l =[]\n",
    "        inp = []\n",
    "        msk = []\n",
    "        for i in range(len(self.labels)):\n",
    "            l.append(self.labels[i])\n",
    "            inp.append(self.input_ids[i])\n",
    "            msk.append(self.mask[i])\n",
    "\n",
    "        self.dataset['labels'] = l\n",
    "        self.dataset['input_ids'] = inp\n",
    "        self.dataset['attention_mask'] = msk\n",
    "        \n",
    "        self.dataset['mlm'] = self.dataset.progress_apply(self.apply_mlm, axis=1)\n",
    "\n",
    "    def get_graph_from_smiles(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return torch.tensor([[], []], dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    0\n",
    "    \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "    \n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "        \n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        node_feat = torch.cat([x1, x2], dim=-1)\n",
    "    \n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            \n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "    \n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.long)\n",
    "        num_nodes = N\n",
    "        num_edges = M\n",
    "        return node_feat, edge_index, edge_attr, num_nodes, num_edges\n",
    "\n",
    "    def get_augmented_graph_copy(self, node_feat, edge_index, edge_attr, N, M):\n",
    "        num_mask_nodes = max([1, math.floor(self.node_mask_percent * N)])\n",
    "\n",
    "        mask_nodes = random.sample(list(range(N)), num_mask_nodes)\n",
    "\n",
    "        node_feat_new = deepcopy(node_feat)\n",
    "        for atom_idx in mask_nodes:\n",
    "            node_feat_new[atom_idx, :] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        return Data(x=node_feat_new, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    def tokenize(self, item):\n",
    "        return self.tokenizer(item, truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "    def tokenize_descriptors(self, data):\n",
    "        sample = tokenizer.tokenize(data['descriptors'], max_length=512)\n",
    "        \n",
    "        self.labels.append(torch.tensor(sample['input_ids']))\n",
    "        self.mask.append(torch.tensor(sample['attention_mask']))\n",
    "        self.input_ids.append(self.mlm(self.labels[-1].detach().clone()))\n",
    "        \n",
    "        self.input_ids = torch.cat(self.input_ids)\n",
    "        self.mask = torch.cat(self.mask)\n",
    "        \n",
    "        self.labels = torch.cat(self.labels)\n",
    "\n",
    "    def mlm(self, tensor):\n",
    "        rand = torch.rand(tensor.shape)\n",
    "        mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "        for i in range(tensor.shape[0]):\n",
    "            selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "            tensor[i, selection] = 4\n",
    "        return tensor\n",
    "\n",
    "    def apply_mlm(self, sample):\n",
    "        labels = torch.tensor(sample.input_ids)\n",
    "        attention_mask = torch.tensor(sample.attention_mask)\n",
    "        input_ids = torch.tensor(sample.input_ids)\n",
    "        return Data(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset['mlm'].iloc[index], self.dataset['graph_copy1'].iloc[index], self.dataset['graph_copy2'].iloc[index], int(self.dataset['p_np'].iloc[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self):\n",
    "        pass\n",
    "    def len(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d0346-4204-4edd-a947-de80e85a685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MoleculeDataset(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef882144-9701-4c8e-b169-0e11113172b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "    \n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(val_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    \n",
    "train_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=train_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")\n",
    "    \n",
    "eval_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=valid_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")\n",
    "    \n",
    "test_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=test_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920a19e",
   "metadata": {},
   "source": [
    "### Create contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc2bf1-7318-4f29-afdb-8d3b70794bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, device, batch_size, temperature, use_cosine_similarity):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
    "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_similarity_function(self, use_cosine_similarity):\n",
    "        if use_cosine_similarity:\n",
    "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "            return self._cosine_simililarity\n",
    "        else:\n",
    "            return self._dot_simililarity\n",
    "\n",
    "    def _get_correlated_mask(self):\n",
    "        diag = np.eye(2 * self.batch_size)\n",
    "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
    "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
    "        mask = torch.from_numpy((diag + l1 + l2))\n",
    "        mask = (1 - mask).type(torch.bool)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot_simililarity(x, y):\n",
    "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, C, 2N)\n",
    "        # v shape: (N, 2N)\n",
    "        return v\n",
    "\n",
    "    def _cosine_simililarity(self, x, y):\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, 2N, C)\n",
    "        # v shape: (N, 2N)\n",
    "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "        return v\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        representations = torch.cat([zjs, zis], dim=0)\n",
    "\n",
    "        similarity_matrix = self.similarity_function(representations, representations)\n",
    "\n",
    "        # filter out the scores from the positive samples\n",
    "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
    "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
    "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
    "\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        logits = logits.abs() + 0.0001\n",
    "        logits = torch.log(logits)\n",
    "        logits /= self.temperature\n",
    "        \n",
    "        labels = torch.zeros(2 * self.batch_size).to(self.device).long()\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        return loss / (2 * self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fdd5f",
   "metadata": {},
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "from torch import nn\n",
    "\n",
    "if config['graph_model_type'] == 'gin':\n",
    "    from graph_models.models.ginet_old import GINet as GraphModel\n",
    "elif config['graph_model_type'] == 'gcn':\n",
    "    from graph_models.models.gcn import GCN as GraphModel\n",
    "else:\n",
    "    raise ValueError('GNN model is not defined in config.')\n",
    "\n",
    "class MolecularBertGraph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularBertGraph, self).__init__()\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        roberta_config = roberta_config = RobertaConfig(\n",
    "            vocab_size=30_522,\n",
    "            max_position_embeddings=514,\n",
    "            hidden_size=768,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1\n",
    "        )\n",
    "        \n",
    "        self.bert = RobertaForMaskedLM(roberta_config).to(device)\n",
    "\n",
    "        self.graph_model = GraphModel(**config['graph_model']).to(device)\n",
    "        # self.graph_model = self._load_graph_pretrained_weights(self.graph_model)\n",
    "\n",
    "        self.out_graph_linear = torch.nn.Linear(2 * config['graph_model']['feat_dim'], \n",
    "                                                768, bias=True)\n",
    "\n",
    "        self.out_graph_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_graph_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn2_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_bert = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "        \n",
    "        self.bn2_bert = nn.BatchNorm1d(768)\n",
    "        \n",
    "        # contrastive loss\n",
    "        self.nt_xent_criterion = NTXentLoss(device, self.batch_size, **config['ntxent_loss'])\n",
    "\n",
    "    def forward(self, bert_batch, graph_batch1, graph_batch2):\n",
    "        bert_output = self.bert(input_ids=bert_batch['input_ids'].view(self.batch_size, -1), \n",
    "                                 attention_mask=bert_batch['attention_mask'].view(self.batch_size, -1),\n",
    "                                 labels=bert_batch['labels'].view(self.batch_size, -1), output_hidden_states=True)\n",
    "        bert_loss = bert_output.loss\n",
    "        bert_emb = bert_output.hidden_states[0][:, 0, :] # take emb for CLS token\n",
    "\n",
    "        graph_loss, hidden_states_1, hidden_states_2 = self.graph_step(graph_batch1, graph_batch2)\n",
    "        #graph_loss.backward()\n",
    "        \n",
    "        graph_emb = self.out_graph_linear(torch.cat((hidden_states_1, hidden_states_2), dim=-1))\n",
    "\n",
    "        graph_emb_projected1 = self.out_graph_projection1(graph_emb)\n",
    "        \n",
    "        graph_emb_projected_bn1 = self.bn1_graph(graph_emb_projected1)\n",
    "\n",
    "        graph_emb_projected2 = self.out_graph_projection2(torch.nn.functional.relu(graph_emb_projected_bn1))\n",
    "        \n",
    "        graph_emb_projected_bn2 = self.bn2_graph(graph_emb_projected2)\n",
    "        \n",
    "        #bert projections:\n",
    "        bert_emb_projected1 = self.out_bert_projection1(bert_emb)\n",
    "        \n",
    "        bert_emb_projected_bn1 = self.bn1_bert(bert_emb_projected1)\n",
    "\n",
    "        bert_emb_projected2 = self.out_bert_projection2(torch.nn.functional.relu(bert_emb_projected_bn1))\n",
    "        \n",
    "        bert_emb_projected_bn2 = self.bn2_bert(bert_emb_projected2)\n",
    "\n",
    "        bimodal_loss = self.nt_xent_criterion(bert_emb_projected_bn2, graph_emb_projected_bn2)\n",
    "        return bert_loss, graph_loss, bimodal_loss, graph_emb_projected_bn2, bert_emb_projected_bn2\n",
    "\n",
    "    def graph_step(self, xis, xjs):\n",
    "        # get the representations and the projections\n",
    "        ris, zis = self.graph_model(xis)  # [N,C]\n",
    "    \n",
    "        # get the representations and the projections\n",
    "        rjs, zjs = self.graph_model(xjs)  # [N,C]\n",
    "    \n",
    "        # normalize projection feature vectors\n",
    "        zis = torch.nn.functional.normalize(zis, dim=1)\n",
    "        zjs = torch.nn.functional.normalize(zjs, dim=1)\n",
    "\n",
    "        loss = self.nt_xent_criterion(zis, zjs)\n",
    "        return loss, ris, rjs\n",
    "        \n",
    "    def _load_graph_pretrained_weights(self, model):\n",
    "        try:\n",
    "            checkpoints_folder = os.path.join('graph_models', 'ckpt', config['load_graph_model'], 'checkpoints')\n",
    "            print(os.path.join(checkpoints_folder, 'model.pth'))\n",
    "            state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'))\n",
    "            \n",
    "            model.load_state_dict(state_dict)\n",
    "            print(\"Loaded pre-trained model with success.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Pre-trained weights not found. Training from scratch.\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"bert_bimodal\",\n",
    "    name=\"BBBP QSAR-all metrics-gin\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training (with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea2e1e-8d54-4f03-914f-6b6328b5da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd36a8-610d-4760-a984-79bf17e5b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_checkpoints_folder = os.path.join('ckpts')\n",
    "dir_name = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = os.path.join(model_checkpoints_folder, dir_name)\n",
    "_save_config_file(config, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df3a5c-965c-4a66-bd38-a8158398ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularPropertiesClassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularPropertiesClassification, self).__init__()\n",
    "\n",
    "        self.test = MolecularBertGraph().to(device)\n",
    "        self.test.load_state_dict(torch.load('gin_9.pth'))\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(768 * 2, 768, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(768, 2, bias=True)\n",
    "\n",
    "    def forward(self, b, g1, g2):\n",
    "        l1, l2, l3, graph_emb, bert_emb = self.test(b, g1, g2)\n",
    "        \n",
    "        first_linear_out = self.linear1(torch.cat((graph_emb, bert_emb), dim=-1))\n",
    "        logits = self.linear2(torch.nn.functional.sigmoid(first_linear_out))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca196530-c8af-4581-b213-affedb0109db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MolecularPropertiesClassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3826d7c-9d81-4c05-87f3-9bbde4986f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877c631-0ca2-4527-a2d3-009122d28b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = config['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), float(config['init_lr']), \n",
    "    weight_decay=eval(config['weight_decay'])\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['epochs']-config['warm_up'], \n",
    "    eta_min=0, last_epoch=-1\n",
    ")\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c431e43-da30-40d3-bff0-4e06bfde9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    train_tqdm = tqdm(train_dataloader, unit=\"batch\")\n",
    "    train_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    \n",
    "    model.train()\n",
    "    for (bert_batch, graph_batch1, graph_batch2, targets) in train_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "        targets = targets.clone().detach().to(device)\n",
    "\n",
    "        logits = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "        loss = loss_func(logits.view(-1, 2), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        pred_labels = torch.argmax(logits, dim=-1)\n",
    "        true_labels = targets\n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_tqdm.set_postfix(loss=loss.item())\n",
    "    return loss_sum / len(train_dataloader), total_pred_labels, total_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc579384-660b-4979-80ce-199a40c953e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop():\n",
    "    eval_tqdm = tqdm(eval_dataloader, unit=\"batch\")\n",
    "    eval_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    for (bert_batch, graph_batch1, graph_batch2, targets) in eval_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "        targets = targets.clone().detach().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "        loss = loss_func(logits.view(-1, 2), targets.view(-1))\n",
    "\n",
    "        pred_labels = torch.argmax(logits, dim=-1)\n",
    "        true_labels = targets\n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        eval_tqdm.set_postfix(loss=loss.item())\n",
    "    return loss_sum / len(eval_dataloader), total_pred_labels, total_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf8e101-1ffe-4b6d-9f2d-2d44b5d4d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop():\n",
    "    test_tqdm = tqdm(test_dataloader, unit=\"batch\")\n",
    "    test_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    for (bert_batch, graph_batch1, graph_batch2, targets) in test_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "        targets = targets.clone().detach().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "        loss = loss_func(logits.view(-1, 2), targets.view(-1))\n",
    "\n",
    "        pred_labels = torch.argmax(logits, dim=-1)\n",
    "        true_labels = targets\n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        test_tqdm.set_postfix(loss=loss.item())\n",
    "    return loss_sum / len(test_tqdm), total_pred_labels, total_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1df98b-7cfb-4841-87a5-36f767b6823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5b225-316d-426b-ae91-7bbbdbaf55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423f7a4-5757-43b0-8f17-23f1fd58da2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iter = 0\n",
    "valid_n_iter = 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch_counter in range(num_epoch):\n",
    "    loss, total_pred_labels, total_true_labels = train_loop()\n",
    "    \n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "    wandb.log({\"loss/train\": loss})\n",
    "    wandb.log({\"accuracy/train\": accuracy_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"f1/train\": f1_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"precision/train\": precision_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"recall/train\": recall_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_auc_score/train\": roc_auc_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"roc_curve/train\": roc_curve(total_true_labels, total_pred_labels)})\n",
    "\n",
    "\n",
    "    loss, total_pred_labels, total_true_labels = eval_loop()\n",
    "\n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "    wandb.log({\"loss/eval\": loss})\n",
    "    wandb.log({\"accuracy/validation\": accuracy_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"f1/validation\": f1_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"precision/validation\": precision_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"recall/validation\": recall_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_auc_score/validation\": roc_auc_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"roc_curve/validation\": roc_curve(total_true_labels, total_pred_labels)})\n",
    "    \n",
    "    if loss < best_valid_loss:\n",
    "        best_valid_loss = loss\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_qsar_bbbp.pth'))\n",
    "    \n",
    "    if (epoch_counter + 1) % config['save_every_n_epochs'] == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_qsar_bbbp_{}.pth'.format(str(epoch_counter))))\n",
    "\n",
    "    if epoch_counter >= config['warm_up']:\n",
    "                scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da77412-b5e1-4f61-ae28-c1d8fc873e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, total_pred_labels, total_true_labels = test_loop()\n",
    "\n",
    "total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "wandb.log({\"roc_auc_score/test\": roc_auc_score(total_true_labels, total_pred_labels)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8fc7c-b423-49f7-84d7-c5b71352e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, total_pred_labels, total_true_labels = eval_loop()\n",
    "\n",
    "total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "wandb.log({\"roc_auc_score/valid\": roc_auc_score(total_true_labels, total_pred_labels)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c93d09-66e8-491a-8c97-88abcd2f4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(total_true_labels, total_pred_labels, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
